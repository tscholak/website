cv:
  name: "Torsten Scholak"
  location: "Montréal, QC, Canada"
  website: "https://tscholak.github.io"
  social_networks:
    - network: "GitHub"
      username: "tscholak"
    - network: "LinkedIn"
      username: "tscholak"
    - network: "X"
      username: "tscholak"
    - network: "Google Scholar"
      username: "BgkjtKgAAAAJ"
  sections:
    summary:
      - "AI research and engineering leader turning GPU compute into shipped LLMs. Head of ServiceNow's Foundation Model Lab and co-founder of the ServiceNow Language Model (SLAM) initiative, delivering open-weight models like Apriel-5B and Apriel-Nemotron-15B for efficient enterprise use. I excel at leading small, high-agency teams, setting clear strategy, and executing at scale."
    education:
      - institution: "University of Freiburg"
        area: "Theoretical & Mathematical Physics"
        degree: "Ph.D."
        location: "Freiburg, Germany"
        start_date: "2008-12"
        end_date: "2011-12"
        highlights:
          - "magna cum laude"
      - institution: "University of Bayreuth"
        area: "Theoretical & Mathematical Physics"
        degree: "M.S."
        location: "Bayreuth, Germany"
        start_date: "2002-12"
        end_date: "2008-12"
        highlights:
          - "GPA: 1.2 (German system, 1.0 is best)"
    experience:
      - company: "ServiceNow Research"
        position: "Lead Scientist & Team Lead"
        location: "Montréal, QC, Canada"
        start_date: "2024-01-01"
        highlights:
          - "Shipped the open-weights Apriel models, achieving state-of-the-art results on enterprise benchmarks while optimizing for serving cost and latency, recognized by NVIDIA CEO Jensen Huang in the ServiceNow Knowledge 2025 keynote."
          - "Co-founded the cross-org ServiceNow Language Model (SLAM) initiative, aligning 20+ engineers and researchers on a shared vision for efficient multimodal agentic LLMs that became the Apriel model family."
          - "Pivoted the Foundation Model Lab from fundamental research to platform-first objectives, shipping 3 model families in 12 months (Mixtral-8x7B multilingual upgrade, Apriel-5B, Apriel-15B) delivering serving-cost and latency wins for the ServiceNow platform."
          - Rolled out an "upgrade-don't-retrain" strategy, retrofitting new capabilities (SSMs, multi-token prediction, masked diffusion, pruning, depth/MoE upcycling) via targeted distillation and continual pre-training, adding features at less than 5 % of full token budget and shrinking experiment cycles from weeks to days.
          - "Set research direction that cut inference latency 4x (2x multi-token prediction + 2x SSMs), slashing serving cost and response time."
          - "Established critical-path discipline and tight platform alignment, minimizing low-impact work and ensuring on-time delivery, even during a five-week parental leave."
          - "Coached engineers and researchers into technical leads and fostered a culture of accountability."
          - "Drove fast resourcing decisions through concise executive updates and clear priority setting."
      - company: "ServiceNow Research"
        position: "Applied Research Scientist → Staff Research Scientist"
        location: "Montréal, QC, Canada"
        start_date: "2021-01"
        end_date: "2023-12"
        highlights:
          - "Pioneered small language models (SLMs) for agentic tasks on the ServiceNow platform."
          - "Core contributor to the TapeAgents LLM agent development framework."
      - company: "Element AI (acquired by ServiceNow)"
        position: "Applied Research Scientist - Research & AI Core"
        location: "Montréal, QC, Canada"
        start_date: "2017-10"
        end_date: "2020-12"
        highlights:
          - "Tech-led NLP group: set roadmap, ran stand-ups, mentored interns, and liaised with product on dialog systems, text-to-code, summarization, QA."
          - "Invented grammar-constrained decoding speed-ups (cited 440+ times)."
          - "Built SOTA text-to-SQL model (PICARD) that lead the Spider leaderboard for months."
      - company: "Unata (acquired by Instacart)"
        position: "Data Science Engineer"
        location: "Toronto, ON, Canada"
        start_date: "2016-08"
        end_date: "2017-09"
        highlights:
          - "Re-engineered recommender stack in Scala/Spark."
          - "Delivered 3.5h Bayesian ML tutorial at PyCon 2017 (12k+ views, recording at https://www.youtube.com/watch?v=fR5Wvb86-IU)."
      - company: "University of Toronto"
        position: "Postdoctoral Researcher"
        location: "Toronto, ON, Canada"
        start_date: "2011-06"
        end_date: "2016-03"
        highlights:
          - "Pioneered quantum coherent-control interferometry."
          - "Ran HPC workloads on SciNet (5 k+ CPU cores, NVIDIA Teslas) to simulate quantum systems."
    publications:
      - title: "PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models"
        authors:
          - "***T. Scholak***"
          - N. Schucher
          - D. Bahdanau
        date: "2021-11"
        journal: EMNLP 2021
        doi: 10.18653/v1/2021.emnlp-main.779
        summary: "Incremental parsing keeps generated SQL valid; >400 citations, SOTA on Spider/CoSQL at release."
      - title: "Multilingual Code Retrieval Without Paired Data: A New Benchmark and Experiments"
        authors:
          - J. Monteiro
          - "***T. Scholak***"
          - V. Mehta
          - D. Vazquez
          - C. Pal
        date: "2023-03"
        journal: ICLR 2023 DL4C workshop
        summary: "Introduced two cross-lingual code-text datasets; contrastive training beats GPT-4 baselines on 6 languages."
      - title: "StarCoder 2 and The Stack v2: The Next Generation"
        authors:
          - "BigCode Team"
        date: "2024-02"
        doi: 10.48550/arXiv.2402.19173
        summary: "StarCoder 2 is a 15B parameter model trained on 1.8 trillion tokens of code, achieving SOTA on code generation benchmarks at the time of release."
      - title: "TapeAgents: a Holistic Framework for Agent Development and Optimization"
        authors:
          - D. Bahdanau
          - N. Gontier
          - G. Huang,
          - E. Kamalloo
          - R. Pardinas
          - A. Piché
          - "***T. Scholak***"
          - O. Shliazhko
          - J. P. Tremblay
          - K. Ghanem
          - S. Parikh
          - M. Tiwari
          - Q. Vohra
        date: "2024-12"
        doi: 10.48550/arXiv.2412.08445
        summary: "Introduces a framework for LLM agent development, including a library of agentic tasks, evaluation metrics, and optimization techniques."
      - title: "Unifying Autoregressive and Diffusion-Based Sequence Generation"
        authors:
          - N. Fathi
          - "***T. Scholak***"
          - P.-A. Noel
        date: "2025-04"
        doi: 10.48550/arXiv.2504.06416
        summary: "Shows diffusion can share weights with an AR LM; 2x speed-up at equal perplexity."
    projects:
      - name: "Apriel Model Family - Co-lead & Technical Architect"
        summary: "5-15 B parameter open-weights LLMs optimized for enterprise agentic tasks and efficient inference."
        highlights:
          - "Co-led technical design and release of Apriel-5B and Apriel-Nemotron-15B-Thinker."
          - "Apriel-5B-Instruct outperforms OLMo-2-7B and Mistral-Nemo-12B across average benchmarks; competitive with LLaMA 3.1 8B, with strong results in math and reasoning (AIME-24/25, GPQA, MATH-500)."
          - "Apriel-Nemotron-15B-Thinker achieves state-of-the-art on BFCL, Enterprise RAG, MT-Bench, MixEval, IFEval, Multi-Challenge, MBPP while using 40% fewer tokens than 30B+ models like QWQ-32B."
          - "Publicly recognized by NVIDIA CEO Jensen Huang during model announcement in ServiceNow Knowledge 2025 keynote."
      - name: "Fast-LLM - Strategic Lead"
        summary: "Opinionated, high-performance PyTorch-based distributed model-training framework for trillion-token LLM pre-training."
        highlights:
          - "Provided strategic direction and priority setting; day-to-day maintenance handled by core contributors."
          - "Supports dense, MoE, and hybrid SSM architectures; integrates 3D parallelism, ZeRO, and FlashAttention."
          - "Adopted by ServiceNow Foundation Model Lab and platform teams for production use."
          - "Used to train billion-parameter models on trillions of tokens on NVIDIA DGX SuperPOD clusters with 500+ GPUs."
          - "Trained Apriel-5B and Apriel-Nemotron-15B-Thinker."
      - name: "Deep Learning for Code (DL4C) - Co-organizer"
        summary: "Annual workshop on deep learning for code, co-located with ICLR."
        highlights:
          - "Co-organized 2022 and 2023 workshops, including program committee, event organization, and panel hosting."
          - "2022: 30+ submissions, 15 accepted papers, 100+ attendees."
          - "2023: 40+ submissions, 19 accepted papers, 150+ attendees."
      - name: "Hasktorch - Core Contributor"
        summary: "Haskell bindings for PyTorch, enabling GPU-accelerated machine learning in Haskell."
        highlights:
          - "Added compile-time shape/type checking and transformer examples"
          - "Live-coded demo at FP Berlin (8.9k+ views, recording at https://www.youtube.com/watch?v=ZnYa99QoznE&t=1689)."
design:
  theme: "sb2nov"
  page:
    size: "us-letter"
locale:
  language: en
